# -*- coding: utf-8 -*-
"""CCT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lcQn5BGro6Pi6faSKPnCzQ9t6E3vQCdQ
"""

pip install keras --upgrade

import keras
from keras import layers
import matplotlib.pyplot as plt
import numpy as np

## HYPER PARAMETERS ##
pos_emb = True ### Change this to false if you don't want to apply positional embedding after tokenization
conv_layers = 2 ### Number of layers in the convolutional tokenizer

classes = 10 ### Total number of classes in the dataset
input_shape = (32, 32, 3) ### Shape of each input image

projection_dim = 128 ### Dimensions of projection matrix
num_heads = 2 ### Number of attention heads

transformer_units = [projection_dim, projection_dim] ### Specifying transformer units
transformer_layers = 2 ### Number of transformer blocks
stochastic_depth_rate = 0.1 ### Depth rate for regularization

learning_rate = 0.0001
weight_decay = 0.0001
batch_size = 128
num_epochs = 30
image_size = 32

# Loading CIFAR-10 dataset

(X_train, y_train),(X_test, y_test) = keras.datasets.cifar10.load_data()

y_train = keras.utils.to_categorical(y_train, classes)
y_test = keras.utils.to_categorical(y_test, classes)

print("X_train shape: ", X_train.shape)
print("y_train shape: ", y_train.shape)
print("X_test shape: ", X_test.shape)
print("y_test shape: ", y_test.shape)

data_augumentation = keras.Sequential([
    layers.Rescaling(scale = 1.0 / 255),
    layers.RandomCrop(image_size, image_size),
    layers.RandomFlip("horizontal")], name = 'data_augumentation')

# Designing the convolutional tokenizer
class ccttokenizer(layers.Layer):
  def __init__ (self,
                kernel_size = 3, # kernel size for the convolutional layer
                stride = 1, # stride for convolutional layer
                padding = 1,
                pool_kernal_size = 3, # kernel size for the Max Pooling layer
                pool_stride = 2, # stride for the Max Pooling layer
                n_conv_layers = conv_layers,
                n_output_channels = [64,128],
                positional_emb = pos_emb,
                **kwargs): # We are using **kwargs to zip all the arguments and pass it as a blob by unpacking all of it using super function.

    super().__init__(**kwargs) #Unpacking all the zipped arguments from the init function
    # Creating a convolutional network to tokenize the data
    self.model = keras.Sequential()
    for i in range(n_conv_layers):
      self.model.add(layers.Conv2D(n_output_channels[i],
                                   kernel_size,
                                   stride,
                                   padding = "valid",
                                   use_bias = "False",
                                   activation = "relu",
                                   kernel_initializer = "he_normal")) # Convolutional layer
      self.model.add(layers.ZeroPadding2D(padding)) # Zero padding layer
      self.model.add(layers.MaxPooling2D(pool_kernal_size, pool_stride, "same")) # Max Pooling layer
    self.positional_emb = pos_emb
  def call(self, images):
    outputs = self.model(images) # Feeding the images to the convolutional model
    reshaped_outputs = keras.ops.reshape(outputs,(-1, keras.ops.shape(outputs)[1] * keras.ops.shape(outputs)[2], keras.ops.shape(outputs)[-1]))
    return reshaped_outputs # Sequence of vectors

class positional_embedding(keras.layers.Layer):
  def __init__(self,
               sequence_length, # Length of input sequence
               initializer = "glorot_uniform",
               **kwargs):
    super().__init__(**kwargs)
    if sequence_length is None:
      raise ValueError("`sequence_length` must be an Integer, received `None`.")
    self.sequence_length = int(sequence_length)
    self.initializer = keras.initializers.get(initializer) # initiating the initializer

  # Function for returning data configuration
  def get_config(self):
    config = super().get_config()
    config.update({"sequence_length": self.sequence_length,
                   "initializer": keras.initializers.serialize(self.initializer)})
    return config

  def build(self, input_shape):
    feature_size = input_shape[-1] # number of columns
    self.positional_embedding = self.add_weight(name = "embeddings",
                                                shape = [self.sequence_length, feature_size],
                                                initializer = self.initializer,
                                                trainable = True)
    super().build(input_shape)

  def call(self, inputs, start_index = 0):
    shape = keras.ops.shape(inputs)
    feature_len = shape[-1]
    sequence_len = shape[-2]
    # feature_len and sequence_length might be different. So we trim feature_len
    # to same length as sequence
    positional_embedding = keras.ops.convert_to_tensor(self.positional_embedding)
    positional_embedding = keras.ops.slice(positional_embedding, (start_index, 0), (sequence_len, feature_len))

    return keras.ops.broadcast_to(positional_embedding, shape)

  def compute_output(self, input_shape):
    return input_shape

class sequencepooling(layers.Layer):
  def __init__(self):
    super().__init__()
    self.attention = layers.Dense(1)

  def call(self, x):
    attention_weights = keras.ops.softmax(self.attention(x), axis=1) # Applying softmax
    attention_weights = keras.ops.transpose(attention_weights, axes = (0,2,1)) # Transpose on the desired axes
    weighted = keras.ops.matmul(attention_weights, x) # Cross multiplication between the attention weights and input x
    return keras.ops.squeeze(weighted, -2) # Joining tensor arrays along the axis

class stochasticdepth(layers.Layer):
  def __init__(self, drop_prop, **kwargs):
    super().__init__(**kwargs)
    self.drop_prop = drop_prop
    self.seed_generator = keras.random.SeedGenerator(1337)

  def call(self, x, training = None):
    if training:
      keep_prop = 1 - self.drop_prop
      shape = (keras.ops.shape(x)[0],) + (1,) * (len(x.shape) - 1)
      random_tensor = keep_prop + keras.random.uniform(shape, 0, 1, seed = self.seed_generator)
      random_tensor = keras.ops.floor(random_tensor)
      return (x/keep_prop) * random_tensor
    return x

def mlp(x, hidden_units, drop_rate):
  for units in hidden_units:
    x = layers.Dense(units, activation = keras.ops.gelu)(x) # Using gaussian cumulative
    # distribution function
    x = layers.Dropout(drop_rate)(x)
  return x

def create_cct(image_size = image_size,
               input_shape = input_shape,
               num_heads = num_heads,
               projection_dim = projection_dim,
               transformer_units = transformer_units):
  inputs = layers.Input(input_shape)

  # Augmenting the data
  augemented_data = data_augumentation(inputs)

  # Encoding the patches
  cct = ccttokenizer()
  encoded_patches = cct(augemented_data)

  # Applying positional embedding
  if pos_emb:
    sequence_length = encoded_patches.shape[1]
    encoded_patches = encoded_patches + positional_embedding(sequence_length = sequence_length)(encoded_patches)

  # Calculating stochastic depth probabilities
  dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]

  # Creating multiple layers of Transformer blocks
  for i in range(transformer_layers):
    # layer normalization
    x1 = layers.LayerNormalization(epsilon = 1e-5)(encoded_patches)

    # Adding multi head attention layer
    attention = layers.MultiHeadAttention(num_heads = num_heads, key_dim = projection_dim, dropout = 0.1)(x1,x1)
    attention = stochasticdepth(dpr[i])(attention)
    x2 = layers.Add()([attention, encoded_patches])

    # layer normalization
    x3 = layers.LayerNormalization(epsilon = 1e-5)(x2)

    # Adding multi layer perceptron layer
    x3 = mlp(x3, hidden_units = transformer_units, drop_rate = 0.1)
    x3 = stochasticdepth(dpr[i])(x3)
    encoded_patches = layers.Add()([x3,x2])

  # Adding a sequence pooling layer
  representation = layers.LayerNormalization(epsilon = 1e-5)(encoded_patches)
  weight = sequencepooling()(representation)

  # Classifying the outputs
  output_logits = layers.Dense(classes)(weight)

  # Creating model
  model = keras.Model(inputs = inputs, outputs = output_logits)
  return model

def run(model):
  optimizer = keras.optimizers.AdamW(learning_rate = 0.0001, weight_decay = 0.0001)

  model.compile(optimizer = optimizer,
                loss = keras.losses.CategoricalCrossentropy(from_logits = True, label_smoothing = 0.1),
                metrics = [keras.metrics.CategoricalAccuracy(name="accuracy"),
                           keras.metrics.TopKCategoricalAccuracy(5, name = "top-5")])
  history = model.fit(x = X_train, y = y_train, batch_size = batch_size, epochs = num_epochs, validation_split = 0.1)

  return history

cct_model = create_cct()
history = run(cct_model)

print("Training Accuracy with 30 epochs : ", history.history['accuracy'][-1])
print("Training Loss with 30 epochs : ", history.history['loss'][-1])
print("Training Top-5 Accuracy with 30 epochs : ", history.history['top-5'][-1])
print("Validation Accuracy with 30 epochs : ", history.history['val_accuracy'][-1])
print("Validation Loss with 30 epochs : ", history.history['val_loss'][-1])
print("Validation Top-5 Accuracy with 30 epochs : ", history.history['val_top-5'][-1])

