# -*- coding: utf-8 -*-
"""CS_532 - HW4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pSmZKorlD49tqGjemFESGjhBpMjGuacw

# Point Cloud Fusion
"""

pip install open3d

import numpy as np
import cv2
from google.colab.patches import cv2_imshow
import matplotlib.pyplot as plt
import itertools
import open3d as od3

def compute_derivative(input_image):
  # Setting the kernel
  kernel_X = np.array([[-1,0,1],[-1,0,1],[-1,0,1]])
  kernel_Y = np.array([[-1,-1,-1],[0,0,0],[1,1,1]])

  # Padding the input image to compensate with the kernel size.
  input_image_pad = np.pad(input_image, pad_width = 1, mode = 'constant')

  # Creating output storage
  gradx = np.zeros((input_image.shape[0], input_image.shape[1]))
  grady = np.zeros((input_image.shape[0], input_image.shape[1]))

  # Passing the window over the padded image
  for i in range(input_image_pad.shape[0] - (kernel_X.shape[0]-1)):
    for j in range(input_image_pad.shape[1] - (kernel_X.shape[1]-1)):
      gradx[i,j] = np.sum(input_image_pad[i:i+kernel_X.shape[0], j:j+kernel_X.shape[1]] * kernel_X)
      grady[i,j] = np.sum(input_image_pad[i:i+kernel_Y.shape[0], j:j+kernel_Y.shape[1]] * kernel_Y)

  # Computing double derivative
  gradxx = gradx * gradx
  gradyy = grady * grady
  gradxy = gradx * grady

  return gradxx, gradyy, gradxy

def gaussian_filter(input_image, filter_size = 5, sigma = 1):
  # Finding the distance to pad the image for gaussian filter
  center = filter_size//2

  # Allocating storage for filter and the output image
  filter = np.zeros((filter_size, filter_size))
  output_image = np.zeros((input_image.shape[0], input_image.shape[1]))

  # Padding the image
  input_image_pad = np.pad(input_image, pad_width = center, mode='constant')

  # Computing the gaussian filter
  for i in range(filter.shape[0]):
    for j in range(filter.shape[1]):
      x_coord = abs(i - center)
      y_coord = abs(j - center)

      term1 = 1 / (2 * np.pi * (sigma**2))
      term2 = np.exp((-(x_coord ** 2) - (y_coord ** 2)) / (2 * (sigma ** 2)))
      filter[j,i] = term1 * term2

  # Normalizing Gaussian filter
  #filter = filter / np.sum(filter)

  # Passing the gaussian filter over the input image
  for i in range(input_image_pad.shape[0] - (filter.shape[0] - 1)):
    for j in range(input_image_pad.shape[1] - (filter.shape[1] - 1)):
      output_image[i,j] = np.sum(input_image_pad[i : i + filter.shape[0], j : j + filter.shape[1]] * filter)

  return output_image

def harris_corner(input_image):
  # Defining the value for alpha and the window size for non maximum suppression
  alpha = 0.02
  window_size = 3
  # Computing the derivatives
  grad_xx, grad_yy, grad_xy = compute_derivative(input_image)

  # Allocating storage for corner response values
  corner = np.zeros((input_image.shape[0], input_image.shape[1]))


  # Applying gaussian filter on all the derivatives
  grad_xx_gauss = gaussian_filter(grad_xx)
  grad_yy_gauss = gaussian_filter(grad_yy)
  grad_xy_gauss = gaussian_filter(grad_xy)

  # Computing Corner Response for each pixel
  for i in range(grad_xx_gauss.shape[0]):
    for j in range(grad_xx_gauss.shape[1]):
      term1 = grad_xx_gauss[i,j]
      term2 = grad_yy_gauss[i,j]

      corner[i,j] = (term1 * term2) - (alpha * (term1 + term2))

  # Performing non maximum suppression
  corner_response_pad = np.pad(corner, pad_width = 1, mode = 'constant')
  for i in range(corner_response_pad.shape[0] - (window_size - 1)):
    for j in range(corner_response_pad.shape[1] - (window_size - 1)):
      if corner_response_pad[i+1, j+1] != np.max(corner_response_pad[i: i+window_size, j: j+window_size]):
        corner[i,j] = 0

  # Pikcing the top 100 corner responses
  non_zero = []
  output = np.zeros((corner.shape[0], corner.shape[1]))
  count = 0

  for i in range(corner.shape[0]):
    for j in range(corner.shape[1]):
      if corner[i,j] > 0:
        non_zero.append([i,j,int(corner[i,j])])

  non_zero = np.array(non_zero)
  values = non_zero[:, -1]
  sortedind = np.argsort(values)
  sortedind = sortedind[::-1]
  non_zero = non_zero[sortedind]

  for item in non_zero:
    if count < 100:
      x,y = item[0], item[1]
      output[x,y] = item[2]
      count += 1
    else:
      break

  return output

def drawCorners(inputImage, responseFunction):
    fig, ax = plt.subplots()
    ax.imshow(inputImage)
    coordinates = []
    for i in range(inputImage.shape[0]):
        for j in range(inputImage.shape[1]):
            if responseFunction[i, j] > 0:
                coordinates.append((j, i))

    for coord in coordinates:
        circle = plt.Circle(coord, 1, color = 'red', fill = False)
        ax.add_artist(circle)

    ax.set_xlim(0, inputImage.shape[1])
    ax.set_ylim(inputImage.shape[0], 0)

    plt.show()

def corners_2_3D(corners, depthmap):
  # Setting camera intrinsic parameters and scale
  K = np.array([[525.0, 0, 319.5],[0, 525.0, 239.5],[0,0,1]])
  S = 5000
  K_inv = np.linalg.inv(K)
  # Allocating storage for the output
  output = []

  # Iterating through the pixels
  for i in range(corners.shape[0]):
    for j in range(corners.shape[1]):
      if depthmap[i,j] != 0 and corners[i,j] != 0:
        x,y,z = ((1/S) * depthmap[i,j]) * (np.dot(K_inv, np.array([i,j,1])))
        output.append([x,y,z])

  return output

def corner_match_2_3D(corner_matches, depthmap_ref, depthmap_comp):
  # Setting camera intrinsic parameters and scale
  K = np.array([[525.0, 0, 319.5],[0, 525.0, 239.5],[0,0,1]])
  S = 5000
  K_inv = np.linalg.inv(K)

  # Storing 3D locations of the matches points.
  cm_imageref_3D = []
  cm_imagecomp_3D = []

  for point in corner_matches:
    point_refimg = point[:2].astype(int)
    point_compimg = point[2:-1].astype(int)

    # Computing the 3D coordinates for reference image points
    x_ref, y_ref, z_ref = ((1/S) * depthmap_ref[point_refimg[0], point_refimg[1]]) * (np.dot(K_inv, np.array([point_refimg[0],point_refimg[1],1])))
    cm_imageref_3D.append([x_ref, y_ref, z_ref])
    # Computing the 3D coordinates for comparison image points
    x_comp, y_comp, z_comp = ((1/S) * depthmap_comp[point_compimg[0], point_compimg[1]]) * (np.dot(K_inv, np.array([point_compimg[0], point_compimg[1],1])))
    cm_imagecomp_3D.append([x_comp, y_comp, z_comp])

  return np.array(cm_imageref_3D), np.array(cm_imagecomp_3D)

def rank_transform(input_image, window_size = 5):
  # Allocating space for the output image
  output = np.zeros((input_image.shape[0], input_image.shape[1]))

  # Iterating through the image to compute rank transform
  for i in range(input_image.shape[0] - window_size):
    for j in range(input_image.shape[1] - window_size):
      # Extracting windows from input image
      currentwindow = input_image[i: i + window_size, j: j + window_size]
      currentwindowravel = currentwindow.ravel()
      # Computing rank of the center element in the window
      rank = np.where(currentwindowravel < currentwindowravel[(window_size ** 2) // 2])[0].shape[0]
      output[i+2, j+2] = rank
  return output

def corner_matching(image1_rt, image1_corner, image2_rt, image2_corner):
  # Isolating the x and y co ordinates of both corner images.
  image1_loc = []
  image2_loc = []
  for i in range(image1_corner.shape[0]):
    for j in range(image1_corner.shape[1]):
      if image1_corner[i,j] > 0:
        image1_loc.append([i,j])
      if image2_corner[i,j] > 0:
        image2_loc.append([i,j])

  # for each corner location computing SAD
  window_size = 11
  distance = np.inf
  distances = []
  for point2 in image2_loc:
    closest_point = [0,0]
    i,j = point2

    patch1 = image1_rt[i - (window_size // 2): i + (window_size // 2) + 1, j - (window_size // 2): j + (window_size // 2) + 1]
    if patch1.size != (window_size ** 2):
      continue
    else:
      for point1 in image1_loc:
        k,l = point1
        patch2 = image2_rt[k - (window_size // 2): k + (window_size // 2) + 1, l - (window_size // 2): l + (window_size // 2) + 1]
        if patch2.size != (window_size ** 2):
          continue
        else:
          absdifference = abs(patch2 - patch1).sum()
          if absdifference < distance:
            distance = absdifference
            #closest_point[0], closest_point[1] = k,l
            closest_point = [k,l]
    distances.append([i,j,closest_point[0],closest_point[1],distance])

  # Sorting distances
  distances = np.array(distances)
  values = distances[:,-1]
  sorted_ind = np.argsort(values)
  sorted_ind = sorted_ind[::-1]
  distances = distances[sorted_ind]

  return distances[:10]

def check_collinearity(A,B,C):
  AB = B-A
  AC = C-A

  crossproduct = np.cross(AB, AC)

  if np.allclose(crossproduct, [0,0,0]):
    return True
  else:
    return False

def distance(p1, p2):
  return np.sqrt((p1[0]-p2[0]) ** 2 + (p1[1]-p2[1]) ** 2 + (p1[2]-p2[2]) ** 2)

def RANSAC(cm_image_ref_3D, cm_image_comp_3D, response_ref_3D, response_comp_3D):
  # generating combinations of 3D points from the 10 corner points
  index_combinations = np.array(list(itertools.combinations(range(10), 3)))
  # Variables
  inlier_count = 0
  inlier_indices = []
  best_R = None
  best_T = None

  # Looping through each combination of coordinates
  for combination in index_combinations:
    i,j,k = combination[:]

    if check_collinearity(cm_image_ref_3D[i], cm_image_ref_3D[j], cm_image_ref_3D[k]) == False and check_collinearity(cm_image_comp_3D[i], cm_image_comp_3D[j], cm_image_comp_3D[k]) == False:

      P11, P12, P13 = cm_image_comp_3D[i], cm_image_comp_3D[j], cm_image_comp_3D[k]
      P21, P22, P23 = cm_image_ref_3D[i], cm_image_ref_3D[j], cm_image_ref_3D[k]

      v11 = P11 - P12
      v12 = P12 - P13
      v21 = P21 - P22
      v22 = P22 - P23

      r1_prime = [v21, v22, np.cross(v21, v22)] * np.linalg.inv([v11, v12, np.cross(v11, v12)])
      U,S,V = np.linalg.svd(r1_prime)
      r1 = np.cross(U, V)
      t1 = P21 - np.dot(r1, P11)

      current_inlier_counts = 0
      current_inlier = []
      for m in range(len(response_comp_3D)):
        T = np.dot(r1, response_comp_3D[m]) + t1
        diff = distance(T, response_ref_3D[m])
        if diff <= 0.005:

          current_inlier_counts += 1
          current_inlier.append(m)
      if current_inlier_counts - 3 > inlier_count:

        inlier_count = current_inlier_counts - 3
        best_R = r1
        best_T = t1
      if len(current_inlier) > len(inlier_indices):
        inlier_indices.clear()
        inlier_indices.append(current_inlier)
  return best_R, best_T

def coronat_opus(image1, depth1, image2, depth2, image3, depth3, best_R12, best_T12, best_R23, best_T23):
  rgb_values = []
  point_cloud = []

  # Setting camera intrinsic parameters and scale
  K = np.array([[525.0, 0, 319.5],[0, 525.0, 239.5],[0,0,1]])
  S = 5000
  K_inv = np.linalg.inv(K)

  # Translating the image 1 3D points to match image 2 3D points and assigning RGB values
  for i in range(image1.shape[0]):
    for j in range(image1.shape[1]):
      if depth1[i,j] != 0:
        x,y,z = ((1/S) * depth1[i,j]) * (np.dot(K_inv, np.array([i,j,1])))
        r,g,b = image1[i,j]
        x_translated, y_translated, z_translated = np.dot(best_R12, [x,y,z]) + best_T12

        point_cloud.append([x_translated, y_translated, z_translated])
        rgb_values.append([r,g,b])

  # Storing image 2 3D points in pointcloud
  for i in range(image2.shape[0]):
    for j in range(image2.shape[1]):
      if depth2[i,j] != 0:
        x,y,z = ((1/S) * depth2[i,j]) * (np.dot(K_inv, np.array([i,j,1])))
        r,g,b = image1[i,j]

        point_cloud.append([x,y,z])
        rgb_values.append([r,g,b])

  # Translating the image 3 3D points to match image 2 3D points and assigning RGB values
  for i in range(image3.shape[0]):
    for j in range(image3.shape[1]):
      if depth3[i,j] != 0:
        x,y,z = ((1/S) * depth3[i,j]) * (np.dot(K_inv, np.array([i,j,1])))
        r,g,b = image1[i,j]
        x_translated, y_translated, z_translated = np.dot(best_R23, [x,y,z]) + best_T23

        point_cloud.append([x_translated, y_translated, z_translated])
        rgb_values.append([r,g,b])

  # Creating ply files to visualize the fused pointcloud
  pcd = od3.geometry.PointCloud()
  pcd.colors = od3.utility.Vector3dVector(np.array(rgb_values).astype(float)/ 255.0)
  pcd.points = od3.utility.Vector3dVector(point_cloud)
  od3.io.write_point_cloud("PC_Fusion.ply", pcd, write_ascii = True)

img1 = cv2.imread("rgb1.png")
img2 = cv2.imread("rgb2.png")
img3 = cv2.imread("rgb3.png")
# Loading images in gray scale
image1 = cv2.imread("rgb1.png", cv2.IMREAD_GRAYSCALE)
image2 = cv2.imread("rgb2.png", cv2.IMREAD_GRAYSCALE)
image3 = cv2.imread("rgb3.png", cv2.IMREAD_GRAYSCALE)
# Loading depth map in gray scale
depth1 = cv2.imread("depth1.png", cv2.IMREAD_GRAYSCALE)
depth2 = cv2.imread("depth2.png", cv2.IMREAD_GRAYSCALE)
depth3 = cv2.imread("depth3.png", cv2.IMREAD_GRAYSCALE)
print("Data loaded with 3RGB images and their corresponding depth maps")

# Isolating the corners
print("\nComputing Harris Corner Detection")
image1_corner = harris_corner(image1)
image2_corner = harris_corner(image2)
image3_corner = harris_corner(image3)
print("Harris Corner Detection completed")

# Converting the corners into 3D coordinates
print("\nConverting Corner points to 3D coordinates")
image1_3D = corners_2_3D(image1_corner, depth1)
image2_3D = corners_2_3D(image2_corner, depth2)
image3_3D = corners_2_3D(image3_corner, depth3)
print("Conversion completed")

# Rank Transform on all the images
print("\nComputing Rank Transform")
image1_rt = rank_transform(image1)
image2_rt = rank_transform(image2)
image3_rt = rank_transform(image3)
print("Rank Transform completed")

# Matching corners between images 1
print("\nPreforming Corner Matching")
cm_top10_12 = corner_matching(image1_rt, image1_corner, image2_rt, image2_corner)
cm_top10_23 = corner_matching(image3_rt, image3_corner, image2_rt, image2_corner)
print("Corner Matching completed")

# Computing corner matched points in 3D coordinates
print("\nConverting Corner Matched points into 3D coordinates")
cm_image12_3D, cm_image1_3D = corner_match_2_3D(cm_top10_12, depth2, depth1)
cm_image23_3D, cm_image3_3D = corner_match_2_3D(cm_top10_23, depth2, depth3)
print("Corner Matched point 3D coversion completed")

# Running Ransac to find the best rotation and translation on the comparison images.
print("\nRunning RANSAC")
best_R12, best_T12 = RANSAC(cm_image12_3D, cm_image1_3D, image2_3D, image1_3D)
best_R23, best_T23 = RANSAC(cm_image23_3D, cm_image3_3D, image2_3D, image3_3D)
print("RANSAC Operation to find the best Rotation and Translation completed")

# Creating a ply file
coronat_opus(img1, depth1, img2, depth2, img3, depth3, best_R12, best_T12, best_R23, best_T23)
print("\n3D File Generated")